{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1f04e7",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7605c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans1=\"\"\"40%\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43469e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'40%'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357d1d6",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfadf0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans2=\"\"\"The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in how they handle features and are\n",
    "best suited to different types of data:\n",
    "\n",
    "1. Bernoulli Naive Bayes\n",
    "Feature Representation: Bernoulli Naive Bayes is designed for binary or boolean features, where each feature represents the \n",
    "presence or absence of a particular attribute. Each feature can only take values 1 (present) or 0 (absent).\n",
    "Use Case: This model is best suited for text classification tasks where you only care about the presence or absence of a word,\n",
    "rather than its frequency. For example, it might be useful for spam detection, where we are only interested in whether specific\n",
    "keywords are present in an email.\n",
    "Probability Calculation: The model calculates the probability of a document belonging to a class based on whether each feature \n",
    "is present or absent, using a Bernoulli (binomial) distribution.\n",
    "2. Multinomial Naive Bayes\n",
    "Feature Representation: Multinomial Naive Bayes is designed for features that represent frequencies or counts of events. Each \n",
    "feature can take on non-negative integer values, representing how often a particular attribute (e.g., a word) appears in an \n",
    "instance (e.g., a document).\n",
    "Use Case: This model is commonly used for text classification tasks where the frequency of words (or other features) is \n",
    "important, such as in document classification, sentiment analysis, and topic categorization.\n",
    "Probability Calculation: The model calculates probabilities based on the frequency of features using a multinomial \n",
    "distribution.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd3acae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in how they handle features and are\\nbest suited to different types of data:\\n\\n1. Bernoulli Naive Bayes\\nFeature Representation: Bernoulli Naive Bayes is designed for binary or boolean features, where each feature represents the \\npresence or absence of a particular attribute. Each feature can only take values 1 (present) or 0 (absent).\\nUse Case: This model is best suited for text classification tasks where you only care about the presence or absence of a word,\\nrather than its frequency. For example, it might be useful for spam detection, where we are only interested in whether specific\\nkeywords are present in an email.\\nProbability Calculation: The model calculates the probability of a document belonging to a class based on whether each feature \\nis present or absent, using a Bernoulli (binomial) distribution.\\n2. Multinomial Naive Bayes\\nFeature Representation: Multinomial Naive Bayes is designed for features that represent frequencies or counts of events. Each \\nfeature can take on non-negative integer values, representing how often a particular attribute (e.g., a word) appears in an \\ninstance (e.g., a document).\\nUse Case: This model is commonly used for text classification tasks where the frequency of words (or other features) is \\nimportant, such as in document classification, sentiment analysis, and topic categorization.\\nProbability Calculation: The model calculates probabilities based on the frequency of features using a multinomial \\ndistribution.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a70825b",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c470f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans3=\"\"\"In Bernoulli Naive Bayes, missing values are typically treated as an absence of a feature (i.e., a 0) rather than as \n",
    "unknown data. Here’s how it handles this situation:\n",
    "\n",
    "Binary Assumption: Bernoulli Naive Bayes assumes that each feature can be either present (1) or absent (0). If a feature is \n",
    "missing in a dataset, it usually implies that the feature does not appear in that instance, so it is treated as 0.\n",
    "\n",
    "Defaulting to Absence: Since the model relies on binary data and interprets a missing feature as \"not present,\" it doesn’t \n",
    "require imputation or special handling for missing values. The model will consider the absence of the feature (i.e., assigning\n",
    "a 0) in the probability calculation without needing adjustments.\n",
    "\n",
    "Impact on Probability Calculation: When calculating probabilities, each feature contributes based on its presence or absence.\n",
    "If a feature is missing, the model assumes it contributes a likelihood based on a 0 (absence) in the feature vector. \n",
    "This approach works well when missing features can reasonably be interpreted as absent.\n",
    "\n",
    "However, if missing values are genuinely unknown rather than absent, treating them as 0 could lead to biased predictions. \n",
    "In such cases, data preprocessing might be needed to handle missing values more appropriately before using Bernoulli Naive \n",
    "Bayes.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d774eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Bernoulli Naive Bayes, missing values are typically treated as an absence of a feature (i.e., a 0) rather than as \\nunknown data. Here’s how it handles this situation:\\n\\nBinary Assumption: Bernoulli Naive Bayes assumes that each feature can be either present (1) or absent (0). If a feature is \\nmissing in a dataset, it usually implies that the feature does not appear in that instance, so it is treated as 0.\\n\\nDefaulting to Absence: Since the model relies on binary data and interprets a missing feature as \"not present,\" it doesn’t \\nrequire imputation or special handling for missing values. The model will consider the absence of the feature (i.e., assigning\\na 0) in the probability calculation without needing adjustments.\\n\\nImpact on Probability Calculation: When calculating probabilities, each feature contributes based on its presence or absence.\\nIf a feature is missing, the model assumes it contributes a likelihood based on a 0 (absence) in the feature vector. \\nThis approach works well when missing features can reasonably be interpreted as absent.\\n\\nHowever, if missing values are genuinely unknown rather than absent, treating them as 0 could lead to biased predictions. \\nIn such cases, data preprocessing might be needed to handle missing values more appropriately before using Bernoulli Naive \\nBayes.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ans3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3c61a8",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d392f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans4=\"\"\"Yes, Gaussian Naive Bayes can be used for multi-class classification. In fact, Naive Bayes classifiers, including\n",
    "Gaussian Naive Bayes, are well-suited to multi-class classification because of their probabilistic nature. Here’s how it \n",
    "works:\n",
    "\n",
    "Class Probabilities: Gaussian Naive Bayes calculates the probability of each class given the input features using Bayes'\n",
    "theorem. For each class, it computes the likelihood that a given instance belongs to that class based on the Gaussian (normal)\n",
    "distribution of each feature within the class.\n",
    "\n",
    "One-Versus-All Approach: In multi-class classification, Gaussian Naive Bayes computes these probabilities separately for each \n",
    "class and selects the class with the highest posterior probability as the predicted class. This approach is inherently suitable\n",
    "for handling multiple classes without any additional modifications.\n",
    "\n",
    "Independent Distributions for Each Class: For each feature, Gaussian Naive Bayes assumes that the feature values for each class\n",
    "are distributed according to a Gaussian (normal) distribution. It calculates the mean and variance of each feature within each\n",
    "class independently, enabling the model to generalize well to multi-class data.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
